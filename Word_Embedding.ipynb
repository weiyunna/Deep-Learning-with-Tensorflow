{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embedding",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weiyunna/Deep-Learning-with-Tensorflow/blob/master/Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ETI9wvMv2DSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro to Word Embedding"
      ]
    },
    {
      "metadata": {
        "id": "gTcOYbi22O2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial introduces word embeddings. It contains complete code to train word embeddings from scratch on a small dataset, and to visualize these embeddings using the** Embedding Projector** (shown in the image below)."
      ]
    },
    {
      "metadata": {
        "id": "W3x8Bm0b2adR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Represent text as numbers"
      ]
    },
    {
      "metadata": {
        "id": "y4y-hpQR2n_J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing we must do come up with a strategy to convert strings to numbers (or to \"**vectorize**\" the text) before feeding it to the model. In this section, we will look at three strategies for doing so."
      ]
    },
    {
      "metadata": {
        "id": "uxR8Z-fGbQ_K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### One-hot encodings"
      ]
    },
    {
      "metadata": {
        "id": "qxQsHeztblU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a first idea, we might \"one-hot\" encode each word in our vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, we will create a zero vector with `length equal to the vocabulary`, then place a one in the index that corresponds to the word. This approach is shown in the following diagram."
      ]
    },
    {
      "metadata": {
        "id": "pcqlrWCmb42C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To create a vector that contains the encoding of the sentence, we could then concatenate the one-hot vectors for each word.\n",
        "\n",
        "Key point: This approach is inefficient. A one-hot encoded vector is **sparse** (meaning, most indicices are zero). Imagine we have 10,000 words in the vocabulary. To one-hot encode each word, we would create a vector where 99.99% of the elements are zero."
      ]
    },
    {
      "metadata": {
        "id": "R5kBYUqScDlv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode each word with a unique number"
      ]
    },
    {
      "metadata": {
        "id": "j5SblSUNcMlZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A second approach we might try is to encode each word using a unique number. Continuing the example above, we could assign 1 to \"cat\", 2 to \"mat\", and so on. We could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. This appoach is efficient. Instead of a sparse vector, we now have a dense one (where all elements are full)."
      ]
    },
    {
      "metadata": {
        "id": "e78tEUmlci-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are two downsides to this approach, however:\n",
        "\n",
        "* The integer-encoding is arbitrary (it does not capture any relationship between words).\n",
        "\n",
        "* An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because different words may have a similar encoding, this feature-weight combination is not meaningful."
      ]
    },
    {
      "metadata": {
        "id": "K-2jzwOidAvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "DmoGIJlzdX0Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word embeddings give us a way to use an efficient, dense representation in which **similar words have a similar encoding**. \n",
        "\n",
        "Importantly, we do not have to specify this encoding by hand. \n",
        "\n",
        "An embedding is **a dense vector of floating point values ** (the length of the vector is a parameter you specify). \n",
        "\n",
        "Instead of specifying the values for the embedding manually, they are **trainable parameters** (weights learned by the model during training, in the same way a model learns weights for a dense layer). \n",
        "\n",
        "It is common to see word embeddings that **8-dimensional** (for small datasets), up to **1024-dimensions **when working with large datasets. \n",
        "\n",
        "A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn."
      ]
    },
    {
      "metadata": {
        "id": "C46dwXDVeCP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Above is a diagram for a word embedding. \n",
        "\n",
        "Each word is represented as a 4-dimensional vector of floating point values. \n",
        "\n",
        "Another way to think of an embedding is as **\"lookup table\"**. \n",
        "\n",
        "After these weights have been learned, we can encode each word by looking up the dense vector it corresponds to in the table."
      ]
    },
    {
      "metadata": {
        "id": "ShDTrRAneV0j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using the Embedding layer"
      ]
    },
    {
      "metadata": {
        "id": "FltWol00ejwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keras makes it easy to use word embeddings. Let's take a look at the **Embedding** layer."
      ]
    },
    {
      "metadata": {
        "id": "BSpGKGTN2GQ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1ad71adb-3c3b-4ca8-d943-2f0f4f519f83"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# The Embedding layer takes at least two arguments:\n",
        "# the number of possible words in the vocabulary, here 1000 (1 + maximum word index),\n",
        "# and the dimensionality of the embeddings, here 32.\n",
        "embedding_layer = layers.Embedding(1000, 32)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 79.9MB 299kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 419kB 15.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EoKmLISCgKUz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). \n",
        "\n",
        "The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer."
      ]
    },
    {
      "metadata": {
        "id": "a-3l0XxBgbY7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). \n",
        "\n",
        "During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode **similarities** between words (as they were learned for the specific problem your model is trained on)."
      ]
    },
    {
      "metadata": {
        "id": "AVJvUWCegx8c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As **input**, the Embedding layer takes a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers. \n",
        "\n",
        "It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes (32, 10) (batch of 32 sequences of length 10) or (64, 15) (batch of 64 sequences of length 15).\n",
        "\n",
        "All sequences in a batch must have the same length, so sequences that are shorter than others should be **padded** with zeros, and sequences that are longer should be **truncated**."
      ]
    },
    {
      "metadata": {
        "id": "JVzfLMv7h5ub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As output, the embedding layer returns a **3D** floating point tensor, of shape (**samples, sequence_length, embedding_dimensionality**). \n",
        "\n",
        "Such a 3D tensor can then be processed by a RNN layer, or can simply be flattened or pooled and processed by a Dense layer. We will show the first approach in this tutorial, and you can refer to the Text Classification with an RNN to learn the second."
      ]
    },
    {
      "metadata": {
        "id": "abiJym7FieFf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learning Embeddings from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "Q1uyD60eimWI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will train a sentiment classifier on IMDB movie reviews. \n",
        "\n",
        "In the process, we will learn embeddings from scratch. \n",
        "\n",
        "We will move quickly through the code that downloads and preprocesses the dataset (see this tutorial for more details)."
      ]
    },
    {
      "metadata": {
        "id": "w1XtQRh6gTgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "10b12426-720d-4169-9b0f-faf627a4da14"
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "imdb = keras.datasets.imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JcuO-gEnjTNI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As imported, the text of reviews is integer-encoded (each integer represents a specific word in a dictionary)."
      ]
    },
    {
      "metadata": {
        "id": "Y6FrPDffjlim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7463bf13-2a81-4da9-f63b-2e25feb017c9"
      },
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TnkbHerajsaw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert the integers back to words"
      ]
    },
    {
      "metadata": {
        "id": "osCFfpvuj6lx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It may be useful to know how to convert integers back to text. Here, we'll create a helper function to query **a dictionary object that contains the integer to string mapping:**"
      ]
    },
    {
      "metadata": {
        "id": "kYmY1DMWjnHJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "76f6aace-d76e-4fc8-d0d3-c5f9bbbfd4b1"
      },
      "cell_type": "code",
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "decode_review(train_data[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "SaWiJXVFlQ2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_index\n",
        "reverse_word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6j76hl-Ql5lz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Movie reviews can be different lengths. We will use the **pad_sequences ** function to standardize the lengths of the reviews."
      ]
    },
    {
      "metadata": {
        "id": "_9RisArqlXMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "maxlen = 500\n",
        "\n",
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=maxlen)\n",
        "\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fRWYlVugmKTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's inspect the first padded review.\n",
        "\n",
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmZ0TyFWmV08",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Create a simple model"
      ]
    },
    {
      "metadata": {
        "id": "Glo_DAL8mpIs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use the Keras Sequential API to define our model.\n",
        "\n",
        "* The first layer is an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. **The resulting dimensions are: (batch, sequence, embedding)`.**\n",
        "\n",
        "* Next, a **GlobalAveragePooling1D** layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
        "\n",
        "* This fixed-length output vector is piped through a **fully-connected (Dense)** layer with 16 hidden units.\n",
        "\n",
        "* The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a **probability** (or confidence level) that the review is positive."
      ]
    },
    {
      "metadata": {
        "id": "KVa7UwJQmdVK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "7264e420-7f67-46e7-8594-f094163111cf"
      },
      "cell_type": "code",
      "source": [
        "embedding_dim=16\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dense(16, activation='relu'),\n",
        "  layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 16)           160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,289\n",
            "Trainable params: 160,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eih7vulMnpQO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "metadata": {
        "id": "r0wQ8m3lnkZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1110
        },
        "outputId": "84136f6f-cbe8-4c21-901d-fa768144d052"
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    epochs=30,\n",
        "    batch_size=512,\n",
        "    validation_split=0.2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/30\n",
            "20000/20000 [==============================] - 2s 111us/sample - loss: 0.6927 - accuracy: 0.5207 - val_loss: 0.6920 - val_accuracy: 0.5654\n",
            "Epoch 2/30\n",
            "20000/20000 [==============================] - 2s 93us/sample - loss: 0.6908 - accuracy: 0.6351 - val_loss: 0.6889 - val_accuracy: 0.5826\n",
            "Epoch 3/30\n",
            "20000/20000 [==============================] - 2s 95us/sample - loss: 0.6864 - accuracy: 0.6606 - val_loss: 0.6823 - val_accuracy: 0.7256\n",
            "Epoch 4/30\n",
            "20000/20000 [==============================] - 2s 98us/sample - loss: 0.6770 - accuracy: 0.7405 - val_loss: 0.6688 - val_accuracy: 0.7284\n",
            "Epoch 5/30\n",
            "20000/20000 [==============================] - 2s 100us/sample - loss: 0.6595 - accuracy: 0.7645 - val_loss: 0.6463 - val_accuracy: 0.7784\n",
            "Epoch 6/30\n",
            "20000/20000 [==============================] - 2s 101us/sample - loss: 0.6314 - accuracy: 0.7903 - val_loss: 0.6136 - val_accuracy: 0.7868\n",
            "Epoch 7/30\n",
            "20000/20000 [==============================] - 2s 95us/sample - loss: 0.5949 - accuracy: 0.8011 - val_loss: 0.5759 - val_accuracy: 0.7902\n",
            "Epoch 8/30\n",
            "20000/20000 [==============================] - 2s 96us/sample - loss: 0.5529 - accuracy: 0.8188 - val_loss: 0.5356 - val_accuracy: 0.8100\n",
            "Epoch 9/30\n",
            "20000/20000 [==============================] - 2s 97us/sample - loss: 0.5106 - accuracy: 0.8363 - val_loss: 0.4975 - val_accuracy: 0.8268\n",
            "Epoch 10/30\n",
            "20000/20000 [==============================] - 2s 95us/sample - loss: 0.4699 - accuracy: 0.8522 - val_loss: 0.4635 - val_accuracy: 0.8406\n",
            "Epoch 11/30\n",
            "20000/20000 [==============================] - 2s 96us/sample - loss: 0.4334 - accuracy: 0.8618 - val_loss: 0.4336 - val_accuracy: 0.8498\n",
            "Epoch 12/30\n",
            "20000/20000 [==============================] - 2s 97us/sample - loss: 0.4017 - accuracy: 0.8705 - val_loss: 0.4094 - val_accuracy: 0.8562\n",
            "Epoch 13/30\n",
            "20000/20000 [==============================] - 2s 95us/sample - loss: 0.3745 - accuracy: 0.8781 - val_loss: 0.3888 - val_accuracy: 0.8614\n",
            "Epoch 14/30\n",
            "20000/20000 [==============================] - 2s 97us/sample - loss: 0.3512 - accuracy: 0.8838 - val_loss: 0.3721 - val_accuracy: 0.8650\n",
            "Epoch 15/30\n",
            "20000/20000 [==============================] - 2s 96us/sample - loss: 0.3325 - accuracy: 0.8898 - val_loss: 0.3584 - val_accuracy: 0.8688\n",
            "Epoch 16/30\n",
            "20000/20000 [==============================] - 2s 97us/sample - loss: 0.3151 - accuracy: 0.8924 - val_loss: 0.3521 - val_accuracy: 0.8656\n",
            "Epoch 17/30\n",
            "20000/20000 [==============================] - 2s 96us/sample - loss: 0.3007 - accuracy: 0.8975 - val_loss: 0.3381 - val_accuracy: 0.8738\n",
            "Epoch 18/30\n",
            "20000/20000 [==============================] - 2s 94us/sample - loss: 0.2866 - accuracy: 0.9014 - val_loss: 0.3294 - val_accuracy: 0.8746\n",
            "Epoch 19/30\n",
            "20000/20000 [==============================] - 2s 92us/sample - loss: 0.2748 - accuracy: 0.9060 - val_loss: 0.3230 - val_accuracy: 0.8776\n",
            "Epoch 20/30\n",
            "20000/20000 [==============================] - 2s 94us/sample - loss: 0.2641 - accuracy: 0.9100 - val_loss: 0.3168 - val_accuracy: 0.8796\n",
            "Epoch 21/30\n",
            "20000/20000 [==============================] - 2s 93us/sample - loss: 0.2548 - accuracy: 0.9136 - val_loss: 0.3116 - val_accuracy: 0.8804\n",
            "Epoch 22/30\n",
            "20000/20000 [==============================] - 2s 93us/sample - loss: 0.2466 - accuracy: 0.9147 - val_loss: 0.3072 - val_accuracy: 0.8828\n",
            "Epoch 23/30\n",
            "20000/20000 [==============================] - 2s 94us/sample - loss: 0.2380 - accuracy: 0.9178 - val_loss: 0.3038 - val_accuracy: 0.8844\n",
            "Epoch 24/30\n",
            "20000/20000 [==============================] - 2s 92us/sample - loss: 0.2304 - accuracy: 0.9204 - val_loss: 0.3003 - val_accuracy: 0.8864\n",
            "Epoch 25/30\n",
            "20000/20000 [==============================] - 2s 91us/sample - loss: 0.2234 - accuracy: 0.9227 - val_loss: 0.2988 - val_accuracy: 0.8864\n",
            "Epoch 26/30\n",
            "20000/20000 [==============================] - 2s 92us/sample - loss: 0.2174 - accuracy: 0.9241 - val_loss: 0.2943 - val_accuracy: 0.8884\n",
            "Epoch 27/30\n",
            "20000/20000 [==============================] - 2s 94us/sample - loss: 0.2107 - accuracy: 0.9269 - val_loss: 0.2932 - val_accuracy: 0.8890\n",
            "Epoch 28/30\n",
            "20000/20000 [==============================] - 2s 93us/sample - loss: 0.2049 - accuracy: 0.9288 - val_loss: 0.2913 - val_accuracy: 0.8880\n",
            "Epoch 29/30\n",
            "20000/20000 [==============================] - 2s 93us/sample - loss: 0.1991 - accuracy: 0.9311 - val_loss: 0.2883 - val_accuracy: 0.8890\n",
            "Epoch 30/30\n",
            "20000/20000 [==============================] - 2s 94us/sample - loss: 0.1936 - accuracy: 0.9334 - val_loss: 0.2877 - val_accuracy: 0.8886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eyplTvSln9eW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With this approach our model reaches a validation accuracy of around 88% (note the model is overfitting, training accuracy is significantly higher)."
      ]
    },
    {
      "metadata": {
        "id": "l6MJRAuPnwe6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "16c35f77-4d6c-4599-bfab-d8d40a52a20d"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim((0.5,1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAImCAYAAABzdx3iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7p3ac2g3oJcI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Retrieve the Learned Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "CQSDoOiDoVBN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, let's retrieve the word embeddings learned during training. This will be a matrix of shape (**vocab_size,embedding-dimension**)."
      ]
    },
    {
      "metadata": {
        "id": "4NBds3o9oTfl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fd56bc0-4bf1-4d79-a682-b4a55a55c9c2"
      },
      "cell_type": "code",
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "84DUoi_to7nF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now write the weights to disk. To use the Embedding Projector, we will upload two files in tab separated format: a file of vectors (**containing the embedding**), and a file of meta data (**containing the words**)."
      ]
    },
    {
      "metadata": {
        "id": "NlVD1ZmxovCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inSyClo2pn6u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you are running this tutorial in Colaboratory, you can use the following snippet to download these files to your local machine (or use the file browser, View -> Table of contents -> File browser)."
      ]
    },
    {
      "metadata": {
        "id": "wvE1cA08pk8g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FD4KbDgFqvyF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize the Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "bqZHWPG3q5e1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To visualize our embeddings we will upload them to the embedding projector.\n",
        "\n",
        "Open the Embedding Projector.\n",
        "\n",
        "* Click on \"Load data\".\n",
        "\n",
        "* Upload the two files we created above: vecs.tsv and meta.tsv. T\n",
        "\n",
        "The embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\". \n",
        "\n",
        "Note: your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer."
      ]
    },
    {
      "metadata": {
        "id": "Km2wFYeRsI6W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Next Step"
      ]
    },
    {
      "metadata": {
        "id": "XEcramfIsLtd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial has shown you how to train and visualize word embeddings from scratch on a small dataset.\n",
        "\n",
        "* To learn more about embeddings in Keras we recommend these notebooks by François Chollet.\n",
        "\n",
        "* To learn more about text classification (including the overall workflow, and if you're curious about when to use embeddings vs one-hot encodings) we recommend this practical text classification guide."
      ]
    },
    {
      "metadata": {
        "id": "RjTWn_3gqzRX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}