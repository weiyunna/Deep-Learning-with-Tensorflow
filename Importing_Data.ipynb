{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Importing Data",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weiyunna/Deep-Learning-with-Tensorflow/blob/master/Importing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rOwz4aEieLIY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing Data "
      ]
    },
    {
      "metadata": {
        "id": "9kCtgiW9e13m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The tf.data API enables you to **build complex input pipelines from simple, reusable pieces. **\n",
        "\n",
        "* For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. \n",
        "\n",
        "* The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. \n",
        "\n",
        "\n",
        "The tf.data API makes it easy to deal with large amounts of data, different data formats, and complicated transformations."
      ]
    },
    {
      "metadata": {
        "id": "JJ-McT3qfQGm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The **tf.data** API introduces two new abstractions to TensorFlow:\n",
        "\n",
        "* A **tf.data.Dataset** represents a sequence of elements, in which each element contains one or more Tensor objects. For example, in an image pipeline, an element might be a single training example, with a pair of tensors representing the image data and a label. There are two distinct ways to create a dataset:\n",
        "\n",
        "    * Creating a source (e.g. `Dataset.from_tensor_slices()`) constructs a dataset from one or more `tf.Tensor` objects.\n",
        "\n",
        "    * Applying a transformation (e.g.` Dataset.batch()`) constructs a dataset from one or more` tf.data.Dataset` objects.\n",
        "\n",
        "* A **tf.data.Iterator** provides the main way to extract elements from a dataset. The operation returned by` Iterator.get_next()` yields the next element of a Dataset when executed, and typically acts as the interface between input pipeline code and your model. The simplest iterator is a \"one-shot iterator\", which is associated with a particular Dataset and iterates through it once. For more sophisticated uses, the `Iterator.initializer` operation enables you to reinitialize and parameterize an iterator with different datasets, so that you can, for example, iterate over training and validation data multiple times in the same program."
      ]
    },
    {
      "metadata": {
        "id": "HiZta-xJg2gm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basic mechanics"
      ]
    },
    {
      "metadata": {
        "id": "BmRgH-P9l5_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This section of the guide describes the fundamentals of creating different kinds of `Dataset` and `Iterator` objects, and how to extract data from them.\n",
        "\n",
        "To start an input pipeline, you must define a source. For example, to construct a Dataset from some tensors in memory, you can use `tf.data.Dataset.from_tensors()` or `tf.data.Dataset.from_tensor_slices()`. Alternatively, if your input data are on disk in the recommended TFRecord format, you can construct a` tf.data.TFRecordDataset.`\n",
        "\n",
        "Once you have a Dataset object, you can transform it into a new Dataset by chaining method calls on the` tf.data.Dataset` object. For example, you can apply per-element transformations such as `Dataset.map()` (to apply a function to each element), and multi-element transformations such as` Dataset.batch()`. See the documentation for tf.data.Dataset for a complete list of transformations.\n",
        "\n",
        "The most common way to consume values from a Dataset is to make an `iterator` object that provides access to one element of the dataset at a time (for example, by calling `Dataset.make_one_shot_iterator()`). A tf.data.Iterator provides two operations: `Iterator.initializer`, which enables you to (re)initialize the iterator's state; and `Iterator.get_next()`, which returns tf.Tensor objects that correspond to the symbolic next element. Depending on your use case, you might choose a different type of iterator, and the options are outlined below."
      ]
    },
    {
      "metadata": {
        "id": "8dJGnFiSmqMN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Structures"
      ]
    },
    {
      "metadata": {
        "id": "Wlr4GKs-mtMO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A dataset comprises elements that each have the same structure. An element contains one or more `tf.Tensor` objects, called `components`. Each component has a `tf.DType` representing the type of elements in the tensor, and a `tf.TensorShape` representing the (possibly partially specified) static shape of each element. \n",
        "\n",
        "The `Dataset.output_types `and `Dataset.output_shapes` properties allow you to inspect the inferred types and shapes of each component of a dataset element. The** nested structure** of these properties map to the structure of an element, which may be a single tensor, a tuple of tensors, or a nested tuple of tensors. For example:"
      ]
    },
    {
      "metadata": {
        "id": "O2IOC_BBeOe2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2934f373-1f56-4009-fc70-d22a1a2584fd"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
        "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
        "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
        "\n",
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "   (tf.random_uniform([4]),\n",
        "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
        "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
        "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
        "\n",
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
        "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<dtype: 'float32'>\n",
            "(10,)\n",
            "(tf.float32, tf.int32)\n",
            "(TensorShape([]), TensorShape([Dimension(100)]))\n",
            "(tf.float32, (tf.float32, tf.int32))\n",
            "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h8uJNrtVnpTN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is often convenient to give names to each component of an element, for example if they represent different features of a training example. In addition to **tuples**, you can use **collections.namedtuple** or a **dictionary mapping strings to tensors** to represent a single element of a Dataset."
      ]
    },
    {
      "metadata": {
        "id": "zeo62cEKn008",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "deee73bb-49d7-43b9-9603-d16c77797bff"
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "   {\"a\": tf.random_uniform([4]),\n",
        "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
        "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
        "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': tf.float32, 'b': tf.int32}\n",
            "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AKT2iqGwoG2e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Dataset `transformations` support datasets of any structure. \n",
        "\n",
        "When using the `Dataset.map()`, `Dataset.flat_map()`, and `Dataset.filter()` transformations, which apply a function to each element, the element structure determines the arguments of the function:"
      ]
    },
    {
      "metadata": {
        "id": "lgflopn0oPOJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset1 = dataset1.map(lambda x: ...)\n",
        "\n",
        "dataset2 = dataset2.flat_map(lambda x, y: ...)\n",
        "\n",
        "# Note: Argument destructuring is not available in Python 3.\n",
        "dataset3 = dataset3.filter(lambda x, (y, z): ...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z6dyhQsAoRnu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating an Iterator"
      ]
    },
    {
      "metadata": {
        "id": "EeB5xkxCogsF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once you have built a `Dataset` to represent your input data, the next step is to create an `Iterator` to access elements from that dataset. The `tf.data` API currently supports the following iterators, in increasing level of sophistication:\n",
        "\n",
        "* one-shot,\n",
        "* initializable,\n",
        "* reinitializable, and\n",
        "* feedable."
      ]
    },
    {
      "metadata": {
        "id": "8EumC9OIpDRi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A **one-shot** iterator is the simplest form of iterator, which only supports iterating once through a dataset, with no need for explicit initialization. One-shot iterators handle almost all of the cases that the existing queue-based input pipelines support, but they do not support parameterization. Using the example of `Dataset.range()`:"
      ]
    },
    {
      "metadata": {
        "id": "qbJfcUyToZYA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "dataset = tf.data.Dataset.range(100)\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "for i in range(100):\n",
        "  value = sess.run(next_element)\n",
        "  assert i == value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aP8ds_JgqAnO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note: Currently, one-shot iterators are the only type that is easily usable with an Estimator."
      ]
    },
    {
      "metadata": {
        "id": "5TdeB8_4qXXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An **initializable** iterator requires you to run an explicit iterator.initializer operation before using it. In exchange for this inconvenience, it enables you to parameterize the definition of the dataset, using one or more `tf.placeholder()` tensors that can be fed when you initialize the iterator. Continuing the `Dataset.range()` example:"
      ]
    },
    {
      "metadata": {
        "id": "SFHxgftCpK6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "12902de6-dda7-4627-bfcf-9827fa12be81"
      },
      "cell_type": "code",
      "source": [
        "max_value = tf.placeholder(tf.int64, shape=[])\n",
        "dataset = tf.data.Dataset.range(max_value)\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "# Initialize an iterator over a dataset with 10 elements.\n",
        "sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
        "for i in range(10):\n",
        "  value = sess.run(next_element)\n",
        "  assert i == value\n",
        "\n",
        "# Initialize the same iterator over a dataset with 100 elements.\n",
        "sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
        "for i in range(100):\n",
        "  value = sess.run(next_element)\n",
        "  assert i == value"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UVe5ZA4FqqpF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A **reinitializable** iterator can be initialized from multiple different `Dataset` objects. For example, you might have a training input pipeline that uses random perturbations to the input images to improve generalization, and a validation input pipeline that evaluates predictions on unmodified data. These pipelines will typically use different `Dataset` objects that have the same structure (i.e. the same types and compatible shapes for each component)."
      ]
    },
    {
      "metadata": {
        "id": "2tqnUKgnrZfW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define training and validation datasets with the same structure.\n",
        "training_dataset = tf.data.Dataset.range(100).map(\n",
        "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
        "validation_dataset = tf.data.Dataset.range(50)\n",
        "\n",
        "# A reinitializable iterator is defined by its structure. We could use the\n",
        "# `output_types` and `output_shapes` properties of either `training_dataset`\n",
        "# or `validation_dataset` here, because they are compatible.\n",
        "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
        "                                           training_dataset.output_shapes)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "training_init_op = iterator.make_initializer(training_dataset)\n",
        "validation_init_op = iterator.make_initializer(validation_dataset)\n",
        "\n",
        "# Run 20 epochs in which the training dataset is traversed, followed by the\n",
        "# validation dataset.\n",
        "for _ in range(20):\n",
        "  # Initialize an iterator over the training dataset.\n",
        "  sess.run(training_init_op)\n",
        "  for _ in range(100):\n",
        "    sess.run(next_element)\n",
        "\n",
        "  # Initialize an iterator over the validation dataset.\n",
        "  sess.run(validation_init_op)\n",
        "  for _ in range(50):\n",
        "    sess.run(next_element)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ejqmZASarif7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}