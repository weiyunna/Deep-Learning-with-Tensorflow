{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model Development with Custom Weights\n",
    "This example shows how to retrain a model with custom weights and fine-tune the model with quantization, then deploy the model running on FPGA. Only Windows is supported. We use TensorFlow and Keras to build our model. We are going to use transfer learning, with ResNet50 as a featurizer. We don't use the last layer of ResNet50 in this case and instead add our own classification layer using Keras.\n",
    "\n",
    "The custom wegiths are trained with ImageNet on ResNet50. We will use the Kaggle Cats and Dogs dataset to retrain and fine-tune the model. The dataset can be downloaded here. Download the zip and extract to a directory named 'catsanddogs' under your user directory (\"~/catsanddogs\").\n",
    "\n",
    "Please set up your environment as described in the quick start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "After you train your model in float32, you'll write the weights to a place on disk. We also need a location to store the models that get downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_weights_dir = os.path.expanduser(\"~/custom-weights\")\n",
    "saved_model_dir = os.path.expanduser(\"~/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "Load the files we are going to use for training and testing. By default this notebook uses only a very small subset of the Cats and Dogs dataset. That makes it run relatively quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\100064135/catsanddogs\\PetImages\\Cat\\0.jpg\n",
      "C:\\Users\\100064135/catsanddogs\\PetImages\\Dog\\0.jpg\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import imghdr\n",
    "datadir = os.path.expanduser(\"~/catsanddogs\")\n",
    "\n",
    "cat_files = glob.glob(os.path.join(datadir, 'PetImages', 'Cat', '*.jpg'))\n",
    "dog_files = glob.glob(os.path.join(datadir, 'PetImages', 'Dog', '*.jpg'))\n",
    "\n",
    "# Limit the data set to make the notebook execute quickly.\n",
    "cat_files = cat_files[:64]\n",
    "dog_files = dog_files[:64]\n",
    "\n",
    "# The data set has a few images that are not jpeg. Remove them.\n",
    "cat_files = [f for f in cat_files if imghdr.what(f) == 'jpeg']\n",
    "dog_files = [f for f in dog_files if imghdr.what(f) == 'jpeg']\n",
    "\n",
    "if(not len(cat_files) or not len(dog_files)):\n",
    "    print(\"Please download the Kaggle Cats and Dogs dataset form https://www.microsoft.com/en-us/download/details.aspx?id=54765 and extract the zip to \" + datadir)    \n",
    "    raise ValueError(\"Data not found\")\n",
    "else:\n",
    "    print(cat_files[0])\n",
    "    print(dog_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a numpy array as labels\n",
    "image_paths = cat_files + dog_files\n",
    "total_files = len(cat_files) + len(dog_files)\n",
    "labels = np.zeros(total_files)\n",
    "labels[len(cat_files):] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 32 (94, 2) (32, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split images data as training data and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_labels = np.array([[0,1] if i else [1,0] for i in labels])\n",
    "img_train, img_test, label_train, label_test = train_test_split(image_paths, onehot_labels, random_state=42, shuffle=True)\n",
    "\n",
    "print(len(img_train), len(img_test), label_train.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model\n",
    "We use ResNet50 for the featuirzer and build our own classifier using Keras layers. We train the featurizer and the classifier as one model. The weights trained on ImageNet are used as the starting point for the retraining of our featurizer. The weights are loaded from tensorflow chkeckpoint files.\n",
    "\n",
    "Before passing image dataset to the ResNet50 featurizer, we need to preprocess the input file to get it into the form expected by ResNet50. ResNet50 expects float tensors representing the images in BGR, channel last order. We've provided a default implementation of the preprocessing that you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml\n",
    "import azureml.core\n",
    "import azureml.contrib\n",
    "import azureml.contrib.brainwave.models.utils as utils\n",
    "\n",
    "def preprocess_images():\n",
    "    # Convert images to 3D tensors [width,height,channel] - channels are in BGR order.\n",
    "    in_images = tf.placeholder(tf.string)\n",
    "    image_tensors = utils.preprocess_array(in_images)\n",
    "    return in_images, image_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use Keras layer APIs to construct the classifier. Because we're using the tensorflow backend, we can train this classifier in one session with our Resnet50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_classifier(in_tensor):\n",
    "    from keras.layers import Dropout, Dense, Flatten\n",
    "    K.set_session(tf.get_default_session())\n",
    "    \n",
    "    FC_SIZE = 1024\n",
    "    NUM_CLASSES = 2\n",
    "\n",
    "    x = Dropout(0.2, input_shape=(1, 1, 2048,))(in_tensor)\n",
    "    x = Dense(FC_SIZE, activation='relu', input_dim=(1, 1, 2048,))(x)\n",
    "    x = Flatten()(x)\n",
    "    preds = Dense(NUM_CLASSES, activation='softmax', input_dim=FC_SIZE, name='classifier_output')(x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now every component of the model is defined, we can construct the model. Constructing the model with the project brainwave models is two steps - first we import the graph definition, then we restore the weights of the model into a tensorflow session. Because the quantized graph defintion and the float32 graph defintion share the same node names in the graph definitions, we can initally train the weights in float32, and then reload them with the quantized operations (which take longer) to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model(quantized, starting_weights_directory = None):\n",
    "    from azureml.contrib.brainwave.models import Resnet50, QuantizedResnet50\n",
    "    \n",
    "    # Convert images to 3D tensors [width,height,channel]\n",
    "    in_images, image_tensors = preprocess_images()\n",
    "\n",
    "    # Construct featurizer using quantized or unquantized ResNet50 model\n",
    "    if not quantized:\n",
    "        featurizer = Resnet50(saved_model_dir)\n",
    "    else:\n",
    "        featurizer = QuantizedResnet50(saved_model_dir, custom_weights_directory = starting_weights_directory)\n",
    "\n",
    "\n",
    "    features = featurizer.import_graph_def(input_tensor=image_tensors)\n",
    "    # Construct classifier\n",
    "    preds = construct_classifier(features)\n",
    "    \n",
    "    # Initialize weights\n",
    "    sess = tf.get_default_session()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    featurizer.restore_weights(sess)\n",
    "\n",
    "    return in_images, image_tensors, features, preds, featurizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train Model\n",
    "First we train the model with custom weights but without quantization. Training is done with native float precision (32-bit floats). We load the traing data set and batch the training with 10 epochs. When the performance reaches desired level or starts decredation, we stop the training iteration and save the weights as tensorflow checkpoint files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(files):\n",
    "    \"\"\" Read files to array\"\"\"\n",
    "    contents = []\n",
    "    for path in files:\n",
    "        with open(path, 'rb') as f:\n",
    "            contents.append(f.read())\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(preds, in_images, img_train, label_train, is_retrain = False, train_epoch = 10):\n",
    "    \"\"\" training model \"\"\"\n",
    "    from keras.objectives import binary_crossentropy\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    learning_rate = 0.001 if is_retrain else 0.01\n",
    "        \n",
    "    # Specify the loss function\n",
    "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))   \n",
    "    cross_entropy = tf.reduce_mean(binary_crossentropy(in_labels, preds))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    def chunks(a, b, n):\n",
    "        \"\"\"Yield successive n-sized chunks from a and b.\"\"\"\n",
    "        if (len(a) != len(b)):\n",
    "            print(\"a and b are not equal in chunks(a,b,n)\")\n",
    "            raise ValueError(\"Parameter error\")\n",
    "\n",
    "        for i in range(0, len(a), n):\n",
    "            yield a[i:i + n], b[i:i + n]\n",
    "\n",
    "    chunk_size = 16\n",
    "    chunk_num = len(label_train) / chunk_size\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "    for epoch in range(train_epoch):\n",
    "        avg_loss = 0\n",
    "        for img_chunk, label_chunk in tqdm(chunks(img_train, label_train, chunk_size)):\n",
    "            contents = read_files(img_chunk)\n",
    "            _, loss = sess.run([optimizer, cross_entropy],\n",
    "                                feed_dict={in_images: contents,\n",
    "                                           in_labels: label_chunk,\n",
    "                                           K.learning_phase(): 1})\n",
    "            avg_loss += loss / chunk_num\n",
    "        print(\"Epoch:\", (epoch + 1), \"loss = \", \"{:.3f}\".format(avg_loss))\n",
    "            \n",
    "        # Reach desired performance\n",
    "        if (avg_loss < 0.001):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(preds, in_images, img_test, label_test):\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    from keras.metrics import categorical_accuracy\n",
    "\n",
    "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    accuracy = tf.reduce_mean(categorical_accuracy(in_labels, preds))\n",
    "    contents = read_files(img_test)\n",
    "\n",
    "    accuracy = accuracy.eval(feed_dict={in_images: contents,\n",
    "                                        in_labels: label_test,\n",
    "                                        K.learning_phase(): 0})\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\100064135/models\\rn50\\1.1.3\\resnet50_bw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:55, 48.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss =  0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:44, 46.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 loss =  0.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:37, 45.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 loss =  0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:36, 45.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 loss =  0.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:35, 45.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 loss =  0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:35, 45.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 loss =  0.019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:36, 45.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 loss =  0.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:33, 44.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 loss =  0.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:28, 43.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 loss =  0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:30, 44.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 loss =  0.012\n",
      "Accuracy: 0.84375\n"
     ]
    }
   ],
   "source": [
    "# Launch the training\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "with sess.as_default():\n",
    "    in_images, image_tensors, features, preds, featurizer = construct_model(quantized=False)\n",
    "    train_model(preds, in_images, img_train, label_train, is_retrain=False, train_epoch=10)    \n",
    "    accuracy = test_model(preds, in_images, img_test, label_test)  \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    featurizer.save_weights(custom_weights_dir + \"/rn50\", tf.get_default_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "After training, we evaluate the trained model's accuracy on test dataset with quantization. So that we know the model's performance if it is deployed on the FPGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing trained model with quantization\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\100064135/custom-weights/rn50\n",
      "Accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "with sess.as_default():\n",
    "    print(\"Testing trained model with quantization\")\n",
    "    in_images, image_tensors, features, preds, quantized_featurizer = construct_model(quantized=True, starting_weights_directory=custom_weights_dir)\n",
    "    accuracy = test_model(preds, in_images, img_test, label_test)      \n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Model\n",
    "Sometimes, the model's accuracy can drop significantly after quantization. In those cases, we need to retrain the model enabled with quantization to get better model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:52, 47.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss =  0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:49, 47.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 loss =  0.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:52, 47.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 loss =  0.421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:48, 47.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 loss =  0.359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:53, 48.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 loss =  0.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:51, 48.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 loss =  0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:53, 47.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 loss =  0.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [05:00, 49.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 loss =  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:54, 47.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 loss =  0.164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [04:42, 46.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 loss =  0.149\n",
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "if (accuracy < 0.93):\n",
    "    with sess.as_default():\n",
    "        print(\"Fine-tuning model with quantization\")\n",
    "        train_model(preds, in_images, img_train, label_train, is_retrain=True, train_epoch=10)\n",
    "        accuracy = test_model(preds, in_images, img_test, label_test)        \n",
    "        print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Definition\n",
    "Like in the QuickStart notebook our service definition pipeline consists of three stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 0 variables.\n",
      "Converted 0 variables to const ops.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\100064135/custom-weights/rn50\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "Converted 4 variables to const ops.\n",
      "C:\\Users\\100064135/models\\model_def.zip\n"
     ]
    }
   ],
   "source": [
    "from azureml.contrib.brainwave.pipeline import ModelDefinition, TensorflowStage, BrainWaveStage\n",
    "\n",
    "model_def_path = os.path.join(saved_model_dir, 'model_def.zip')\n",
    "\n",
    "model_def = ModelDefinition()\n",
    "model_def.pipeline.append(TensorflowStage(sess, in_images, image_tensors))\n",
    "model_def.pipeline.append(BrainWaveStage(sess, quantized_featurizer))\n",
    "model_def.pipeline.append(TensorflowStage(sess, features, preds))\n",
    "model_def.save(model_def_path)\n",
    "print(model_def_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "Go to our GitHub repo \"docs\" folder to learn how to create a Model Management Account and find the required information below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UserErrorException",
     "evalue": "We could not find config.json in: C:\\Users\\100064135 or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUserErrorException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7d2665f9a9b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\azureml\\core\\workspace.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(path, auth)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 raise UserErrorException('We could not find config.json in: {} or in its parent directories. '\n\u001b[0;32m    133\u001b[0m                                          \u001b[1;34m'Please provide the full path to the config file or ensure that '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                                          'config.json exists in the parent directories.'.format(normalized_path))\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUserErrorException\u001b[0m: We could not find config.json in: C:\\Users\\100064135 or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories."
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first time the code below runs it will create a new service running your model. If you want to change the model you can make changes above in this notebook and save a new service definition. Then this code will update the running service in place to run the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from azureml.core.image import Image\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.contrib.brainwave import BrainwaveWebservice, BrainwaveImage\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "model_name = \"catsanddogs-resnet50-model\"\n",
    "image_name = \"catsanddogs-resnet50-image\"\n",
    "service_name = \"modelbuild-service\"\n",
    "\n",
    "registered_model = Model.register(ws, model_def_path, model_name)\n",
    "\n",
    "image_config = BrainwaveImage.image_configuration()\n",
    "deployment_config = BrainwaveWebservice.deploy_configuration()\n",
    "    \n",
    "try:\n",
    "    service = Webservice(ws, service_name)\n",
    "    service.delete()\n",
    "    service = Webservice.deploy_from_model(ws, service_name, [registered_model], image_config, deployment_config)\n",
    "    service.wait_for_deployment(True)\n",
    "except WebserviceException:\n",
    "    service = Webservice.deploy_from_model(ws, service_name, [registered_model], image_config, deployment_config)\n",
    "    service.wait_for_deployment(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
